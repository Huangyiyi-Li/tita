"""
Titaæ—¥æŠ¥åˆ†æä¸€ä½“åŒ–æœåŠ¡

åŠŸèƒ½ï¼š
1. WebæœåŠ¡å™¨ - è®¿é—® http://localhost:8080 æŸ¥çœ‹ä»ªè¡¨æ¿
2. å®šæ—¶ä»»åŠ¡ - è‡ªåŠ¨çˆ¬å–å’Œåˆ†ææ—¥æŠ¥
3. Cookieç®¡ç† - åå°ä¿æ´» + å¤±æ•ˆæ—¶è‡ªåŠ¨å¼¹å‡ºæ‰«ç 

ä½¿ç”¨æ–¹å¼ï¼š
python tita_service.py
"""

import json
import sqlite3
import requests
import datetime
import time
import os
import sys
import random
import threading
import webbrowser
from pathlib import Path
from collections import Counter

# Flaskå’ŒAPScheduler
try:
    from flask import Flask, send_file, jsonify, redirect
    from apscheduler.schedulers.background import BackgroundScheduler
except ImportError:
    print("ç¼ºå°‘ä¾èµ–ï¼Œæ­£åœ¨å®‰è£…...")
    os.system("pip install flask apscheduler")
    from flask import Flask, send_file, jsonify, redirect
    from apscheduler.schedulers.background import BackgroundScheduler

# Selenium (ç”¨äºæ‰«ç )
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸ Seleniumæœªå®‰è£…ï¼ŒCookieå¤±æ•ˆæ—¶éœ€æ‰‹åŠ¨åˆ·æ–°")

# ==================== é…ç½® ====================
CONFIG_FILE = 'config.json'
DB_FILE = 'tita_logs.db'
DASHBOARD_FILE = 'è¾“å‡º/daily_report_dashboard.html'
PORT = 8080
SHARED_COOKIE_FILE = r'f:\å…±äº«é…ç½®\tita_cookie.json'  # å…±äº«Cookieæ–‡ä»¶

# ==================== å…¨å±€çŠ¶æ€ ====================
service_status = {
    "last_fetch": None,
    "last_analysis": None,
    "last_keepalive": None,
    "cookie_valid": True,
    "total_logs": 0,
    "running_since": None
}

# è¿›åº¦è·Ÿè¸ªçŠ¶æ€
fetch_progress = {
    "is_running": False,
    "phase": "",           # "idle", "fetching", "analyzing", "generating", "done", "error"
    "current": 0,          # å½“å‰å¤„ç†æ•°
    "total": 0,            # æ€»æ•°
    "current_user": "",    # å½“å‰å¤„ç†çš„ç”¨æˆ·å
    "message": "",         # çŠ¶æ€æ¶ˆæ¯
    "start_time": None,
    "end_time": None
}

# ==================== å·¥å…·å‡½æ•° ====================

def load_shared_cookie():
    """ä»å…±äº«æ–‡ä»¶åŠ è½½Cookie"""
    try:
        if os.path.exists(SHARED_COOKIE_FILE):
            with open(SHARED_COOKIE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('cookie', '')
    except Exception as e:
        log(f"è¯»å–å…±äº«Cookieå¤±è´¥: {e}", "WARNING")
    return None

def save_shared_cookie(cookie_str):
    """ä¿å­˜Cookieåˆ°å…±äº«æ–‡ä»¶"""
    try:
        os.makedirs(os.path.dirname(SHARED_COOKIE_FILE), exist_ok=True)
        data = {
            'cookie': cookie_str,
            'updated_at': datetime.datetime.now().isoformat(),
            'updated_by': 'tita-å¸‚åœº'
        }
        with open(SHARED_COOKIE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        log(f"Cookieå·²åŒæ­¥åˆ°å…±äº«æ–‡ä»¶")
    except Exception as e:
        log(f"ä¿å­˜å…±äº«Cookieå¤±è´¥: {e}", "WARNING")

def load_config():
    """åŠ è½½é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨å…±äº«Cookie"""
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # å°è¯•ä»å…±äº«æ–‡ä»¶åŠ è½½æ›´æ–°çš„Cookie
    shared_cookie = load_shared_cookie()
    if shared_cookie:
        config['headers']['cookie'] = shared_cookie
    
    return config

def save_config(config):
    """ä¿å­˜é…ç½®ï¼ŒåŒæ—¶æ›´æ–°å…±äº«Cookieæ–‡ä»¶"""
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=4)
    
    # åŒæ­¥æ›´æ–°å…±äº«Cookieæ–‡ä»¶
    cookie_str = config.get('headers', {}).get('cookie', '')
    if cookie_str:
        save_shared_cookie(cookie_str)

def log(message, level="INFO"):
    """æ—¥å¿—è®°å½•"""
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")

# ==================== Cookieç®¡ç† ====================

def test_cookie(config):
    """æµ‹è¯•Cookieæ˜¯å¦æœ‰æ•ˆ"""
    url = config['tita_api_url']
    headers = config['headers']
    
    today = datetime.date.today()
    payload = {
        "pageNum": 1, "pageSize": 1, "relation": 0, "summaryType": 0,
        "startTime": f"{today} 00:00:00", "endTime": f"{today} 23:59:59",
        "searchDepartmentIds": [""], "searchUserIds": [""], "searchGroupIds": [""]
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        if response.status_code in [401, 403]:
            return False, "Cookieå·²å¤±æ•ˆ"
        if response.status_code != 200:
            return False, f"HTTP {response.status_code}"
        data = response.json()
        if data.get('Code') != 1:
            return False, data.get('Message', 'APIé”™è¯¯')
        return True, "Cookieæœ‰æ•ˆ"
    except Exception as e:
        return False, str(e)

def refresh_cookie_with_selenium():
    """ä½¿ç”¨Seleniumå¼¹å‡ºæ‰«ç çª—å£åˆ·æ–°Cookie"""
    if not SELENIUM_AVAILABLE:
        log("Seleniumä¸å¯ç”¨ï¼Œè¯·æ‰‹åŠ¨æ›´æ–°Cookie", "ERROR")
        return False
    
    log("Cookieå¤±æ•ˆï¼Œæ­£åœ¨æ‰“å¼€æ‰«ç çª—å£...")
    
    try:
        options = Options()
        options.add_argument('--start-maximized')
        options.add_experimental_option('excludeSwitches', ['enable-automation'])
        
        try:
            driver = webdriver.Chrome(options=options)
        except:
            from webdriver_manager.chrome import ChromeDriverManager
            from selenium.webdriver.chrome.service import Service
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        
        driver.get("https://work-weixin.tita.com")
        log("è¯·ä½¿ç”¨ä¼ä¸šå¾®ä¿¡æ‰«ç ç™»å½•...")
        
        # ç­‰å¾…ç™»å½•æˆåŠŸ
        timeout = 300
        start = time.time()
        while time.time() - start < timeout:
            if any(x in driver.current_url for x in ["/home", "/weixin/pc/home"]):
                log("æ£€æµ‹åˆ°ç™»å½•æˆåŠŸ!")
                time.sleep(3)
                
                # æå–Cookie
                cookies = driver.get_cookies()
                cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                
                # ä¿å­˜åˆ°é…ç½®
                config = load_config()
                config['headers']['cookie'] = cookie_str
                save_config(config)
                
                driver.quit()
                log("Cookieå·²æ›´æ–°!")
                return True
            time.sleep(2)
        
        driver.quit()
        log("æ‰«ç è¶…æ—¶", "ERROR")
        return False
        
    except Exception as e:
        log(f"æ‰«ç åˆ·æ–°å¤±è´¥: {e}", "ERROR")
        return False

def ensure_valid_cookie():
    """ç¡®ä¿Cookieæœ‰æ•ˆï¼Œå¤±æ•ˆæ—¶è‡ªåŠ¨åˆ·æ–°"""
    global service_status
    config = load_config()
    
    valid, msg = test_cookie(config)
    service_status["cookie_valid"] = valid
    
    if not valid:
        log(f"Cookieæ£€æµ‹: {msg}", "WARNING")
        if refresh_cookie_with_selenium():
            service_status["cookie_valid"] = True
            return True
        return False
    return True

# ==================== æ•°æ®çˆ¬å–ä¸åˆ†æ ====================

def fetch_and_analyze_logs(date_str=None):
    """çˆ¬å–å¹¶åˆ†ææ—¥æŠ¥"""
    global service_status, fetch_progress
    
    # åˆå§‹åŒ–è¿›åº¦
    fetch_progress["is_running"] = True
    fetch_progress["phase"] = "fetching"
    fetch_progress["current"] = 0
    fetch_progress["total"] = 0
    fetch_progress["current_user"] = ""
    fetch_progress["message"] = "æ­£åœ¨æ£€æµ‹CookieçŠ¶æ€..."
    fetch_progress["start_time"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    fetch_progress["end_time"] = None
    
    try:
        if not ensure_valid_cookie():
            fetch_progress["phase"] = "error"
            fetch_progress["message"] = "Cookieæ— æ•ˆä¸”åˆ·æ–°å¤±è´¥"
            fetch_progress["is_running"] = False
            log("Cookieæ— æ•ˆä¸”åˆ·æ–°å¤±è´¥ï¼Œè·³è¿‡çˆ¬å–", "ERROR")
            return False
        
        config = load_config()
        
        # é»˜è®¤è·å–æ˜¨å¤©çš„æ•°æ®
        if date_str is None:
            target_date = datetime.date.today() - datetime.timedelta(days=1)
            date_str = str(target_date)
        
        fetch_progress["message"] = f"æ­£åœ¨è·å– {date_str} çš„æ—¥æŠ¥æ•°æ®..."
        log(f"å¼€å§‹çˆ¬å– {date_str} çš„æ—¥æŠ¥...")
        
        # è°ƒç”¨ç°æœ‰çš„çˆ¬å–é€»è¾‘
        import daily_log_aggregator as aggregator
        
        start_time = f"{date_str} 00:00:00"
        end_time = f"{date_str} 23:59:59"
        
        all_logs = aggregator.fetch_logs(config, start_time, end_time)
        if not all_logs:
            fetch_progress["phase"] = "done"
            fetch_progress["message"] = f"æœªè·å–åˆ° {date_str} çš„æ—¥æŠ¥æ•°æ®"
            fetch_progress["is_running"] = False
            log(f"æœªè·å–åˆ° {date_str} çš„æ—¥æŠ¥æ•°æ®")
            return False
        
        filtered = aggregator.filter_logs(all_logs, config['target_departments'])
        fetch_progress["total"] = len(filtered)
        fetch_progress["message"] = f"è·å–åˆ° {len(filtered)} æ¡æ—¥æŠ¥ï¼Œå¼€å§‹AIåˆ†æ..."
        fetch_progress["phase"] = "analyzing"
        log(f"è·å–åˆ° {len(filtered)} æ¡æ—¥æŠ¥")
        
        # åˆ†æå¹¶ä¿å­˜
        conn = aggregator.init_db()
        processed = []
        
        for idx, log_item in enumerate(filtered):
            user_name = log_item.get('publishUser', {}).get('name', 'Unknown')
            user_id = str(log_item.get('publishUser', {}).get('userId', ''))
            dept_name = log_item.get('publishUser', {}).get('departmentName', '')
            feed_id = log_item.get('feedId', '')
            
            # æ›´æ–°è¿›åº¦
            fetch_progress["current"] = idx + 1
            fetch_progress["current_user"] = user_name
            fetch_progress["message"] = f"æ­£åœ¨åˆ†æ: {user_name} ({idx + 1}/{len(filtered)})"
            
            # æå–å†…å®¹
            content_parts = []
            for item in log_item.get('dailyContent', []):
                title = item.get('title', '')
                text = item.get('content', '')
                if text and text.strip():
                    if title == "ä»Šæ—¥ OKR è¿›å±•":
                        try:
                            okr = json.loads(text)
                            text = "\n".join([f"- {r['Name']}" for r in okr.get('Rows', [])])
                        except:
                            pass
                    content_parts.append(f"**{title}**:\n{text}")
            
            full_content = "\n\n".join(content_parts)
            
            log(f"åˆ†æ: {user_name}...")
            analysis = aggregator.analyze_log_content(full_content, config)
            
            db_data = {
                'feed_id': feed_id, 'user_id': user_id, 'user_name': user_name,
                'department': dept_name, 'log_date': date_str, 'content': full_content
            }
            aggregator.save_log_to_db(conn, db_data, analysis)
            processed.append({'original_log': log_item, 'full_content': full_content, 'analysis': analysis})
        
        conn.close()
        
        # ç”ŸæˆæŠ¥å‘Š
        fetch_progress["phase"] = "generating"
        fetch_progress["message"] = "æ­£åœ¨ç”ŸæˆDashboard..."
        aggregator.generate_report(processed, date_str, config)
        
        service_status["last_fetch"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        service_status["last_analysis"] = service_status["last_fetch"]
        service_status["total_logs"] = len(processed)
        
        # é‡æ–°ç”ŸæˆDashboard
        regenerate_dashboard()
        
        # å®Œæˆ
        fetch_progress["phase"] = "done"
        fetch_progress["message"] = f"âœ… å®Œæˆ! å¤„ç† {len(processed)} æ¡æ—¥æŠ¥"
        fetch_progress["end_time"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        fetch_progress["is_running"] = False
        
        log(f"âœ… å®Œæˆ! å¤„ç† {len(processed)} æ¡æ—¥æŠ¥")
        return True
        
    except Exception as e:
        fetch_progress["phase"] = "error"
        fetch_progress["message"] = f"âŒ çˆ¬å–åˆ†æå¤±è´¥: {str(e)}"
        fetch_progress["is_running"] = False
        log(f"çˆ¬å–åˆ†æå¤±è´¥: {e}", "ERROR")
        import traceback
        traceback.print_exc()
        return False

def regenerate_dashboard():
    """é‡æ–°ç”ŸæˆDashboard"""
    try:
        import generate_dashboard
        # è°ƒç”¨generate_dashboardçš„ä¸»é€»è¾‘
        if hasattr(generate_dashboard, 'main'):
            generate_dashboard.main()
        else:
            exec(open('generate_dashboard.py', encoding='utf-8').read())
        log("Dashboardå·²æ›´æ–°")
    except Exception as e:
        log(f"Dashboardç”Ÿæˆå¤±è´¥: {e}", "ERROR")

# ==================== å®šæ—¶ä»»åŠ¡ ====================

def keepalive_job():
    """ä¿æ´»ä»»åŠ¡"""
    global service_status
    config = load_config()
    valid, msg = test_cookie(config)
    service_status["cookie_valid"] = valid
    service_status["last_keepalive"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    if valid:
        log(f"ä¿æ´»æˆåŠŸ: {msg}")
    else:
        log(f"ä¿æ´»å¤±è´¥: {msg}", "WARNING")

def daily_fetch_job():
    """æ¯æ—¥çˆ¬å–ä»»åŠ¡"""
    log("æ‰§è¡Œæ¯æ—¥å®šæ—¶çˆ¬å–...")
    fetch_and_analyze_logs()

def setup_scheduler():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
    scheduler = BackgroundScheduler()
    
    config = load_config()
    keepalive_config = config.get('keepalive', {})
    
    # æ¯æ—¥çˆ¬å–ä»»åŠ¡ - æ—©ä¸Š9:00
    scheduler.add_job(daily_fetch_job, 'cron', hour=9, minute=0, id='daily_fetch')
    
    # ä¿æ´»ä»»åŠ¡ - å·¥ä½œæ—¶é—´å†…éšæœºæ‰§è¡Œ
    start_hour = keepalive_config.get('start_hour', 8)
    end_hour = keepalive_config.get('end_hour', 18)
    attempts = keepalive_config.get('daily_attempts', 3)
    
    # è®¡ç®—ä¿æ´»æ—¶é—´ç‚¹
    if attempts > 0:
        interval = (end_hour - start_hour) / attempts
        for i in range(attempts):
            hour = int(start_hour + i * interval)
            minute = random.randint(0, 59)
            scheduler.add_job(keepalive_job, 'cron', hour=hour, minute=minute, 
                            id=f'keepalive_{i}', jitter=300)  # 5åˆ†é’Ÿéšæœºæ³¢åŠ¨
    
    scheduler.start()
    log(f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨: æ¯æ—¥9:00çˆ¬å–, {attempts}æ¬¡ä¿æ´»({start_hour}:00-{end_hour}:00)")
    return scheduler

# ==================== Flask WebæœåŠ¡ ====================

app = Flask(__name__)

@app.route('/')
def index():
    """é¦–é¡µ - æ˜¾ç¤ºDashboard"""
    if os.path.exists(DASHBOARD_FILE):
        return send_file(DASHBOARD_FILE)
    return """
    <html>
    <head><title>Titaæ—¥æŠ¥åˆ†ææœåŠ¡</title></head>
    <body style="font-family: sans-serif; padding: 40px; text-align: center;">
        <h1>ğŸš€ Titaæ—¥æŠ¥åˆ†ææœåŠ¡</h1>
        <p>Dashboardå°šæœªç”Ÿæˆï¼Œè¯·å…ˆè·å–æ•°æ®</p>
        <a href="/api/fetch" style="padding: 10px 20px; background: #4F46E5; color: white; 
           text-decoration: none; border-radius: 5px;">ç«‹å³è·å–æ˜¨æ—¥æ—¥æŠ¥</a>
        <br><br>
        <a href="/api/status">æŸ¥çœ‹æœåŠ¡çŠ¶æ€</a>
    </body>
    </html>
    """

@app.route('/api/status')
def api_status():
    """æœåŠ¡çŠ¶æ€API"""
    return jsonify(service_status)

@app.route('/api/progress')
def api_progress():
    """è·å–æ‹‰å–è¿›åº¦"""
    return jsonify(fetch_progress)

@app.route('/api/fetch')
def api_fetch():
    """æ‰‹åŠ¨è§¦å‘çˆ¬å–"""
    def do_fetch():
        fetch_and_analyze_logs()
    
    thread = threading.Thread(target=do_fetch)
    thread.start()
    
    return jsonify({"status": "started", "message": "åå°å¼€å§‹çˆ¬å–ï¼Œè¯·ç¨ååˆ·æ–°é¡µé¢"})

@app.route('/api/refresh-cookie')
def api_refresh_cookie():
    """æ‰‹åŠ¨åˆ·æ–°Cookie"""
    def do_refresh():
        refresh_cookie_with_selenium()
    
    thread = threading.Thread(target=do_refresh)
    thread.start()
    
    return jsonify({"status": "started", "message": "æ­£åœ¨æ‰“å¼€æ‰«ç çª—å£..."})

@app.route('/api/keepalive')
def api_keepalive():
    """æ‰‹åŠ¨ä¿æ´»"""
    keepalive_job()
    return jsonify({"status": "done", "cookie_valid": service_status["cookie_valid"]})

# ==================== ä¸»å‡½æ•° ====================

def main():
    global service_status
    
    print("=" * 60)
    print("  ğŸš€ Titaæ—¥æŠ¥åˆ†æä¸€ä½“åŒ–æœåŠ¡")
    print("=" * 60)
    print()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(CONFIG_FILE):
        print(f"âŒ é…ç½®æ–‡ä»¶ {CONFIG_FILE} ä¸å­˜åœ¨!")
        sys.exit(1)
    
    # åˆå§‹åŒ–çŠ¶æ€
    service_status["running_since"] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    scheduler = setup_scheduler()
    
    # åˆå§‹Cookieæ£€æµ‹
    config = load_config()
    valid, msg = test_cookie(config)
    service_status["cookie_valid"] = valid
    
    if not valid:
        print(f"\nâš ï¸ CookieçŠ¶æ€: {msg}")
        print("å°†åœ¨é¦–æ¬¡è®¿é—®æ—¶æç¤ºåˆ·æ–°\n")
    else:
        print(f"\nâœ… CookieçŠ¶æ€: æœ‰æ•ˆ\n")
    
    print(f"ğŸ“¡ WebæœåŠ¡åœ°å€: http://localhost:{PORT}")
    print(f"ğŸ“Š Dashboard: http://localhost:{PORT}/")
    print(f"ğŸ”§ çŠ¶æ€API: http://localhost:{PORT}/api/status")
    print(f"ğŸ”„ æ‰‹åŠ¨åˆ·æ–°: http://localhost:{PORT}/api/fetch")
    print()
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 60)
    
    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    webbrowser.open(f"http://localhost:{PORT}")
    
    # å¯åŠ¨WebæœåŠ¡
    try:
        app.run(host='0.0.0.0', port=PORT, debug=False, use_reloader=False)
    except KeyboardInterrupt:
        print("\n\næ­£åœ¨åœæ­¢æœåŠ¡...")
        scheduler.shutdown()
        print("æœåŠ¡å·²åœæ­¢")

if __name__ == "__main__":
    main()
